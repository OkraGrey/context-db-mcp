Metadata-Version: 2.4
Name: context-db-mcp
Version: 0.1.0
Summary: MCP server that stores and retrieves project context from an OpenAI vector store.
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Requires-Dist: mcp>=0.3.0
Requires-Dist: openai>=1.44.0
Requires-Dist: pydantic>=2.7.0
Requires-Dist: tenacity>=8.2.3
Provides-Extra: dev
Requires-Dist: pytest>=8.3.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.23.6; extra == "dev"

# Context DB MCP

Context DB MCP is a Model Context Protocol server that lets editor-integrated models
such as Cursor or Claude Code store and retrieve project context using an OpenAI
vector store. It ships two tools:

- `ingest_document` – Uploads a plain-text summary, design doc, or task log into an
  OpenAI vector store. The tool automatically stamps useful attributes (e.g.
  `document_id`, `summary`, and `ingested_at`) so the stored chunks remain
  discoverable later.
- `retrieve_relevant_chunks` – Runs a RAG-style similarity search against the same
  vector store and returns the top matching chunks along with similarity scores and
  metadata, ready to be spliced into the model context window.

The goal is to replace repeated `grep` or manual hunting through a codebase with a
lightweight workflow: continuously ingest the most important artifacts and retrieve
them instantly when a related feature needs updates.

## Installation

```bash
pip install -e .
```

### Requirements

- Python 3.11+
- An OpenAI API key with access to the Assistants v2 / Vector Store APIs

## Configuration

Set the following environment variables (add them to a `.env` file if you prefer):

| Variable | Required | Description |
| --- | --- | --- |
| `OPENAI_API_KEY` or `CONTEXT_DB_OPENAI_API_KEY` | ✅ | API key used for all OpenAI requests |
| `CONTEXT_DB_OPENAI_ORGANIZATION` | Optional | OpenAI organization identifier |
| `CONTEXT_DB_OPENAI_PROJECT` | Optional | OpenAI project identifier |
| `CONTEXT_DB_VECTOR_STORE_ID` | Optional | Default vector store ID used when tool calls omit an explicit store |
| `CONTEXT_DB_VECTOR_STORE_NAME` | Optional | If no store ID is provided, the server will look up (or create) a store with this name |
| `CONTEXT_DB_DEFAULT_MAX_RESULTS` | Optional | Fallback `max_results` for retrieval queries (default: 10) |
| `CONTEXT_DB_LOG_LEVEL` | Optional | Log level (`INFO`, `DEBUG`, etc.) |

> **Tip:** Providing `CONTEXT_DB_VECTOR_STORE_NAME` makes it easy to share a single
> store across multiple sessions without tracking IDs manually. The server scans for
> a store with the provided name before creating a new one.

## Running the server

```bash
context-db-mcp
```

By default the server listens on stdio, which is compatible with Cursor, Claude
Code, and other MCP-aware editors. To run in development mode with hot reloads,
leverage your favourite process manager—`uvx`, `honcho`, or similar.

## Editor integration

### Cursor

Add the following block to your `~/.cursor/mcp.json` (create the file if it does not
exist):

```json
{
  "servers": {
    "context-db": {
      "command": "context-db-mcp",
      "env": {
        "OPENAI_API_KEY": "${OPENAI_API_KEY}",
        "CONTEXT_DB_VECTOR_STORE_NAME": "cursor-shared-context"
      }
    }
  }
}
```

Restart Cursor and look for the tools under the MCP integration panel. You can now
call `ingest_document` periodically (for example, after drafting a design summary)
and pull context with `retrieve_relevant_chunks` before starting a related task.

### Claude Code / Other MCP clients

Any MCP-compliant client can connect over stdio. Point the client to the
`context-db-mcp` binary and ensure the environment variables listed above are
available. Claude Code supports `.anthropic/config.json` with the same shape as
Cursor's configuration file shown above.

## Usage patterns

- **Continuous context capture** – Generate short, structured summaries after each
  session (e.g. feature description, touched files, noteworthy interfaces) and call
  `ingest_document` to store them.
- **Targeted retrieval** – When returning to a feature or debugging a regression,
  call `retrieve_relevant_chunks` with a concise natural-language description. The
  tool returns the highest scoring chunks along with metadata to help you open the
  exact files.

The MCP server intentionally avoids a heavy schema to stay flexible. You can supply
custom attributes (tags, sprint IDs, etc.) for filtered retrieval when needed.

## Development

Run the unit tests:

```bash
pytest
```

Linting is handled by your editor or CI; the project keeps dependencies minimal on
purpose.

## Roadmap ideas

- Optional automatic summarisation before ingestion (using GPT-4o-mini or similar)
- Bulk ingestion of entire folders with globbing support
- First-party CLI for seeding a new vector store from an existing knowledge base

Contributions and feedback are welcome!




